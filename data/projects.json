{
  "projects": [
    {
      "id": 1,
      "title": "Micrograd Neural Network Engine",
      "description": "A minimal neural network engine inspired by micrograd, implemented in pure Python with a custom autograd system and MLP modules for educational deep learning experiments.",
      "detailedDescription": "This project is a lightweight neural network engine built from scratch to understand the fundamentals of backpropagation and neural network training. It is inspired by Andrej Karpathy’s micrograd, focusing on simplicity and educational value. The project includes an automatic differentiation engine, basic neural network components (Neuron, Layer, MLP), and a Jupyter notebook demo to train and visualize results. By avoiding heavy frameworks, it emphasizes hands-on learning and clear insights into how gradients and training loops work at the lowest level.",
      "problemStatement": "Most modern deep learning frameworks abstract away the complexities of automatic differentiation and neural network construction. This project tackles the challenge of implementing these core components from scratch to demystify the internals of backpropagation and model training.",
      "approach": "The project started by implementing the Value class in engine.py, which supports scalar values, operations, and automatic differentiation. Next, a neural network module (nn.py) was developed with classes for Neuron, Layer, and MLP. Finally, a Jupyter notebook (test.ipynb) demonstrates how to train an MLP on a toy dataset, showing both forward and backward passes in action.",
      "technologies": [
        "Python",
        "Autograd",
        "Neural Networks",
        "Jupyter Notebook",
        "Matplotlib",
        "Pandas"
      ],
      "image": "./assets/project_images/micrograd_banner.png",
      "liveUrl": null,
      "githubUrl": "https://github.com/bhanuprakash-18/micrograd",
      "featured": false,
      "status": "completed",
      "startDate": "2024-12-20",
      "endDate": "2025-01-15",
      "highlights": [
        "Built a minimal autograd engine supporting basic mathematical operations",
        "Implemented core neural network components: Neuron, Layer, and MLP",
        "Created a training demo notebook for visualization and experimentation",
        "Enabled forward and backward passes without external deep learning libraries",
        "Strengthened understanding of backpropagation and gradient-based optimization"
      ],
      "codeExamples": [
        {
          "title": "Using the MLP",
          "description": "Example of creating and running a simple multi-layer perceptron",
          "language": "python",
          "snippet": "from engine import Value\nfrom nn import MLP\n\n# Create a simple MLP with 3 inputs, two hidden layers of 4 neurons, and 1 output\nmlp = MLP(3, [4, 4, 1])\n\n# Forward pass\noutput = mlp([2.0, 3.0, -1.0])\nprint(output)"
        }
      ]
    },
    {
      "id": 2,
      "title": "Character-Level Language Modeling Playground",
      "description": "A hands-on project for experimenting with character-level language models in PyTorch. Implemented bigram probabilistic models, a single-layer neural network, and an MLP for next-character prediction to generate names from scratch without relying on high-level libraries.",
      "detailedDescription": "This project is a practical exploration of character-level language modeling, inspired by Andrej Karpathy’s MakeMore series. The goal was to deepen understanding of neural networks and sequence modeling by coding models from scratch in PyTorch. The project progressed from simple bigram probability models to a single-layer neural network and then to an MLP that predicts the next character from a multi-character context. By generating names, visualizing learned weights, and experimenting with training loops, the project provided hands-on practice in low-level deep learning implementation.",
      "problemStatement": "Understanding the inner workings of language models requires more than just using pre-trained architectures. Building models from scratch — including data preprocessing, manual forward and backward passes, and training loops — strengthens intuition for how sequence modeling and neural networks function at a fundamental level.",
      "approach": "The project began with a bigram probabilistic model, counting character pairs and sampling from probability distributions. Next, a single-layer neural network was implemented to learn bigram relationships via gradient descent. Finally, an MLP model was built to use three-character context for predicting the next character, significantly improving generation quality. All models were trained and evaluated on a dataset of names.",
      "technologies": [
        "PyTorch",
        "Python",
        "NumPy",
        "Matplotlib",
        "Character-Level Language Models",
        "Neural Networks"
      ],
      "image": "./assets/project_images/makemore.png",
      "liveUrl": null,
      "githubUrl": "https://github.com/bhanuprakash-18/Character-Level-Language-Modeling-Playground",
      "featured": false,
      "status": "completed",
      "startDate": "2024-11-01",
      "endDate": "2024-12-15",
      "highlights": [
        "Implemented bigram probabilistic, bigram neural network, and MLP models from scratch in PyTorch",
        "Practiced writing forward and backward passes manually for educational purposes",
        "Generated realistic name samples by training on a dataset of thousands of names",
        "Visualized learned weight matrices and probability distributions",
        "Strengthened understanding of training loops, gradient descent, and loss functions"
      ],
      "codeExamples": [
        {
          "title": "Manual Training Loop",
          "description": "Basic gradient descent implementation for character-level models",
          "language": "python",
          "snippet": "for k in range(20):\n    ypred = [n(x) for x in xs]\n    loss = sum((yout - ygt)**2 for (yout, ygt) in zip(ypred, ys))\n    for p in n.parameters():\n        p.grad = 0.0\n    loss.backward()\n    for p in n.parameters():\n        p.data += -0.05 * p.grad\n    print(k, loss.data)"
        }
      ]
    },
    {
      "id": 3,
      "title": "5G Network Anomaly Detection System",
      "description": "Developed machine learning models for fault and alarm detection in 5G network operations during my tenure at TCS. Conducted comprehensive EDA and time-series analysis on large-scale telecommunications KPI and alarm datasets to enhance network reliability.",
      "detailedDescription": "This enterprise-level project involved developing an intelligent anomaly detection system for 5G telecommunications networks. Working with terabytes of network performance data, the system could predict potential network failures before they occurred, significantly reducing downtime and improving customer experience. The solution combined multiple machine learning techniques including unsupervised learning for anomaly detection and supervised learning for alarm classification.",
      "problemStatement": "5G networks generate massive amounts of performance data and alarms, making it impossible for human operators to monitor and respond to all potential issues in real-time. Traditional threshold-based monitoring systems produced too many false positives, while critical anomalies often went undetected until they caused significant service disruptions.",
      "approach": "I implemented a multi-layered approach combining statistical process control, isolation forests for outlier detection, and LSTM networks for time-series anomaly detection. The system incorporated domain knowledge through feature engineering and used ensemble methods to reduce false positive rates. Real-time processing was achieved through Apache Kafka streaming architecture.",
      "technologies": [
        "Python",
        "Machine Learning",
        "Time Series Analysis",
        "Anomaly Detection",
        "Apache Kafka",
        "Plotly",
        "Kibana",
        "Power BI",
        "Telecommunications",
        "LSTM",
        "Isolation Forest"
      ],
      "image": "./assets/images/project-3.jpg",
      "liveUrl": null,
      "githubUrl": null,
      "featured": true,
      "status": "completed",
      "startDate": "2023-06-01",
      "endDate": "2024-09-30",
      "highlights": [
        "Reduced network downtime by 65% through predictive ML-powered fault detection",
        "Built real-time KPI monitoring dashboards serving 50+ network engineers",
        "Processed 2TB+ daily telecommunications data with 99.9% accuracy",
        "Implemented ensemble anomaly detection reducing false positives by 80%",
        "Deployed scalable solution across 15 major telecom circles in India"
      ],
      "codeExamples": [
        {
          "title": "LSTM Anomaly Detection",
          "description": "Time-series anomaly detection using LSTM networks",
          "language": "python",
          "snippet": "class NetworkAnomalyDetector:\n    def __init__(self, sequence_length=60, features=10):\n        self.model = Sequential([\n            LSTM(64, return_sequences=True, input_shape=(sequence_length, features)),\n            Dropout(0.2),\n            LSTM(32, return_sequences=False),\n            Dropout(0.2),\n            Dense(features),\n            Dense(1, activation='sigmoid')\n        ])\n        self.model.compile(optimizer='adam', loss='mse')\n    \n    def detect_anomalies(self, data, threshold=0.95):\n        predictions = self.model.predict(data)\n        mse = np.mean(np.square(data - predictions), axis=1)\n        return mse > np.percentile(mse, threshold)"
        }
      ]
    },
    {
      "id": 4,
      "title": "AI-Powered Customer Care Chatbot",
      "description": "Created an intelligent chatbot for internal query handling and troubleshooting using the RASA framework during my work at TCS. The chatbot improved response times and automated common support queries for telecommunications operations.",
      "technologies": [
        "RASA Framework",
        "Python",
        "Natural Language Processing",
        "Chatbot Development",
        "Machine Learning",
        "NLU",
        "Dialogue Management"
      ],
      "image": "./assets/images/project-4.jpg",
      "liveUrl": null,
      "githubUrl": null,
      "featured": false,
      "status": "completed",
      "startDate": "2023-12-01",
      "endDate": "2024-04-30",
      "highlights": [
        "Implemented intelligent query understanding using NLP",
        "Automated troubleshooting for common network issues",
        "Reduced manual support workload significantly",
        "Integrated with existing telecommunications systems"
      ]
    },
    {
      "id": 5,
      "title": "Healthcare Customer-Care Optimization",
      "description": "Engineered a Flask-based web application with integrated RESTful APIs to optimize the customer-care representative matching algorithm at Welfinity Healthcare. Performed comprehensive data cleaning, preprocessing, and transformation on large-scale customer and service datasets.",
      "technologies": [
        "Flask",
        "Python",
        "RESTful APIs",
        "Data Analytics",
        "Pandas",
        "Algorithm Optimization",
        "Healthcare Data",
        "Web Development"
      ],
      "image": "./assets/images/project-5.jpg",
      "liveUrl": null,
      "githubUrl": "https://github.com/bhanuprakash-18/healthcare-optimization",
      "featured": false,
      "status": "completed",
      "startDate": "2022-09-01",
      "endDate": "2022-10-31",
      "highlights": [
        "Optimized customer-care representative matching by 40%",
        "Built comprehensive data processing pipeline",
        "Developed RESTful APIs for seamless integration",
        "Authored detailed technical documentation"
      ]
    },
    {
      "id": 6,
      "title": "AWS DeepRacer Autonomous Racing",
      "description": "Achieved Top 3 finalist position in AWS DeepRacer global competition by training advanced reinforcement learning models. Designed custom reward functions, fine-tuned hyperparameters, and evaluated model performance in virtual simulation tracks using AWS SageMaker.",
      "technologies": [
        "AWS SageMaker",
        "Reinforcement Learning",
        "AWS DeepRacer",
        "Python",
        "Neural Networks",
        "Hyperparameter Tuning",
        "Simulation",
        "Autonomous Systems"
      ],
      "image": "./assets/images/project-6.jpg",
      "liveUrl": null,
      "githubUrl": "https://github.com/bhanuprakash-18/aws-deepracer-models",
      "featured": false,
      "status": "completed",
      "startDate": "2022-03-01",
      "endDate": "2022-12-15",
      "highlights": [
        "Secured Top 3 position in global AWS competition",
        "Designed innovative reward functions for racing optimization",
        "Applied advanced RL techniques for autonomous navigation",
        "Gained expertise in continuous model improvement"
      ]
    }
  ],
  "metadata": {
    "totalProjects": 6,
    "featuredCount": 3,
    "lastUpdated": "2025-08-15T10:30:00Z",
    "version": "2.0.0",
    "categories": [
      "Machine Learning",
      "Deep Learning",
      "Computer Vision",
      "Data Analytics",
      "AI Applications",
      "Healthcare Technology",
      "Telecommunications",
      "Reinforcement Learning"
    ],
    "technologies": [
      "Python",
      "TensorFlow",
      "Keras",
      "PyTorch",
      "Scikit-Learn",
      "Pandas",
      "NumPy",
      "Flask",
      "RASA",
      "AWS SageMaker",
      "Plotly",
      "Power BI",
      "Kibana"
    ]
  }
}
